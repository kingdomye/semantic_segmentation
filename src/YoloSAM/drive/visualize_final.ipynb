{
 "cells": [
  {
   "cell_type": "code",
   "id": "9e867eac-5563-47fe-8b8f-7e32a3ee33ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:58:30.775684Z",
     "start_time": "2025-12-14T18:58:30.626378Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ÂØºÂÖ•‰Ω†È°πÁõÆ‰∏≠ÁöÑÊ®°Âùó\n",
    "from src.YoloSAM.models.sam import SAMModel\n",
    "from src.YoloSAM.utils.config import SAMFinetuneConfig, SAMDatasetConfig # ÂÄüÁî®ConfigÊù•ÂàùÂßãÂåñÊ®°Âûã\n",
    "\n",
    "# =========================================================\n",
    "# 1. ÂÆö‰πâ‰∏Ä‰∏™‰∏ì‰∏öÁöÑ Inference Á±ª\n",
    "# =========================================================\n",
    "class YoloSAMInference:\n",
    "    def __init__(self, yolo_path, sam_path, device='cuda'):\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        # --- 1. Âä†ËΩΩ YOLO ---\n",
    "        print(f\"Loading YOLO from: {yolo_path}\")\n",
    "        self.yolo_model = YOLO(yolo_path)\n",
    "        print(\"‚úÖ YOLO Model Loaded.\")\n",
    "        \n",
    "        # --- 2. Âä†ËΩΩ‰Ω†ÂæÆË∞ÉÁöÑ SAMModel ---\n",
    "        print(f\"Loading Fine-tuned SAM from: {sam_path}\")\n",
    "        self.sam_model = self._load_finetuned_sam(sam_path)\n",
    "        self.sam_model.to(self.device)\n",
    "        self.sam_model.eval() # ÂàáÊç¢Âà∞ËØÑ‰º∞Ê®°Âºè\n",
    "        print(\"‚úÖ Fine-tuned SAM Model Loaded.\")\n",
    "\n",
    "    def _load_finetuned_sam(self, checkpoint_path):\n",
    "        # ‰ΩøÁî®‰∏éËÆ≠ÁªÉÊó∂Áõ∏ÂêåÁöÑÈÖçÁΩÆÊù•ÂàùÂßãÂåñÊ®°ÂûãÈ™®Êû∂\n",
    "        # Ê≥®ÊÑèÔºöËøôÈáåÁöÑ sam_path ÊòØ‰∏∫‰∫ÜÂàùÂßãÂåñ SAMModel Á±ªÔºåÂÆÉ‰∏ç‰ºöË¢´ÂÆûÈôÖÂä†ËΩΩ\n",
    "        config = SAMFinetuneConfig(model_type='vit_b', sam_path='/root/task/checkpoints/sam_vit_b_01ec64.pth')\n",
    "        model = SAMModel(config)\n",
    "        \n",
    "        # Âä†ËΩΩ‰Ω†ËÆ≠ÁªÉÂ•ΩÁöÑÊùÉÈáçÂ≠óÂÖ∏\n",
    "        state_dict = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        # üî• Ê†∏ÂøÉ‰øÆÂ§çÔºöÂ¶ÇÊûúÊùÉÈáçË¢´ÂåÖË£πÂú® 'model_state_dict' ÈáåÔºåÂÖàÊääÂÆÉÂèñÂá∫Êù•\n",
    "        if 'model_state_dict' in state_dict:\n",
    "            print(\"üì¶ Checkpoint format detected, extracting 'model_state_dict'...\")\n",
    "            state_dict = state_dict['model_state_dict']\n",
    "            \n",
    "        # Âä†ËΩΩÊùÉÈáçÂà∞Ê®°ÂûãÈ™®Êû∂\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "\n",
    "    def predict(self, image_path, yolo_conf=0.01, yolo_iou=0.5, image_size=1024):\n",
    "        # --- 1. ÂõæÂÉèÈ¢ÑÂ§ÑÁêÜ ---\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image_pil)\n",
    "        \n",
    "        # Ê®°Êãü Dataset ‰∏≠ÁöÑ Resize\n",
    "        original_shape = image_np.shape[:2]\n",
    "        resized_image_np = cv2.resize(image_np, (image_size, image_size))\n",
    "        \n",
    "        # --- 2. YOLO Êé®ÁêÜ ---\n",
    "        yolo_results = self.yolo_model.predict(resized_image_np, conf=yolo_conf, iou=yolo_iou, verbose=False)\n",
    "        \n",
    "        detected_boxes = []\n",
    "        if yolo_results and len(yolo_results[0].boxes) > 0:\n",
    "            detected_boxes = yolo_results[0].boxes.xyxy.cpu() # Ëé∑ÂèñÊâÄÊúâÊ£ÄÊµãÊ°Ü\n",
    "        \n",
    "        # --- 3. SAM Êé®ÁêÜ (ÂØπÊØè‰∏™Ê£ÄÊµãÊ°Ü) ---\n",
    "        # ÂõæÂÉèÈúÄË¶ÅËΩ¨Êç¢‰∏∫ Tensor: (C, H, W) -> (1, C, H, W)\n",
    "        image_tensor = torch.from_numpy(resized_image_np).permute(2, 0, 1).float() / 255.0\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)\n",
    "\n",
    "        all_masks = []\n",
    "        if len(detected_boxes) > 0:\n",
    "            with torch.no_grad():\n",
    "                for box in detected_boxes:\n",
    "                    prompt_box = box.unsqueeze(0).to(self.device) # (1, 4)\n",
    "                    \n",
    "                    # ‰ΩøÁî®‰∏éËÆ≠ÁªÉÊó∂ÂÆåÂÖ®Áõ∏ÂêåÁöÑ forward ÊñπÊ≥ï\n",
    "                    pred_mask_logits, _ = self.sam_model.forward_one_image(\n",
    "                        image=image_tensor,\n",
    "                        bounding_box=prompt_box,\n",
    "                        is_train=False\n",
    "                    )\n",
    "                    \n",
    "                    pred_mask_prob = torch.sigmoid(pred_mask_logits)\n",
    "                    pred_mask_binary = (pred_mask_prob > 0.5).squeeze().cpu().numpy()\n",
    "                    all_masks.append(pred_mask_binary)\n",
    "\n",
    "        return {\n",
    "            \"original_image\": resized_image_np,\n",
    "            \"detected_boxes\": detected_boxes.numpy() if len(detected_boxes) > 0 else [],\n",
    "            \"predicted_masks\": all_masks\n",
    "        }\n",
    "\n",
    "    # ++++++++++++++++++++ ËøôÊòØÊ≠£Á°ÆÁöÑÊñ∞ÁâàÊú¨ÔºåËØ∑‰ΩøÁî®ÂÆÉ ++++++++++++++++++++\n",
    "    def visualize_results(self, results):\n",
    "        image = results['original_image']\n",
    "        boxes = results['detected_boxes']\n",
    "        masks = results['predicted_masks']\n",
    "    \n",
    "        # ÂàõÂª∫‰∏Ä‰∏™ÂâØÊú¨Áî®‰∫éÁªòÂà∂ÔºåÈÅøÂÖç‰øÆÊîπÂéüÂßãÊï∞ÊçÆ\n",
    "        vis_image = image.copy()\n",
    "    \n",
    "        if masks:\n",
    "            # Â∞ÜÊâÄÊúâ mask ÂêàÂπ∂Êàê‰∏Ä‰∏™Âçï‰∏ÄÁöÑÂ∏ÉÂ∞îÊé©Á†Å\n",
    "            combined_mask = np.zeros_like(masks[0], dtype=bool)\n",
    "            for mask in masks:\n",
    "                # Á°Æ‰øù mask ÊòØÂ∏ÉÂ∞îÁ±ªÂûã\n",
    "                combined_mask = np.logical_or(combined_mask, mask.astype(bool))\n",
    "    \n",
    "            # üî• Ê†∏ÂøÉ‰øÆÂ§çÔºöÂàõÂª∫‰∏Ä‰∏™ÂΩ©Ëâ≤ÁöÑË¶ÜÁõñÂ±Ç\n",
    "            # ÂÆö‰πâÈ¢úËâ≤ (R, G, B)ÔºåÊ≥®ÊÑè OpenCV ‰ΩøÁî® BGR È°∫Â∫è\n",
    "            color_bgr = (0, 255, 0)\n",
    "            \n",
    "            # Â∞ÜÂ∏ÉÂ∞îÊé©Á†ÅËΩ¨Êç¢‰∏∫ uint8 Ê†ºÂºè (0 Êàñ 255)\n",
    "            binary_mask_uint8 = combined_mask.astype(np.uint8) * 255\n",
    "            \n",
    "            # ‰ΩøÁî® findContours ÊâæÂà∞Êé©Á†ÅÁöÑËΩÆÂªìÔºåÁªòÂà∂ËΩÆÂªìÁ∫øÊØîÂ°´ÂÖÖÊõ¥Ê∏ÖÊô∞\n",
    "            contours, _ = cv2.findContours(binary_mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(vis_image, contours, -1, color_bgr, thickness=2) # ÁªòÂà∂ËΩÆÂªì\n",
    "    \n",
    "            # Â¶ÇÊûú‰Ω†Êõ¥ÂñúÊ¨¢ÂçäÈÄèÊòéÂ°´ÂÖÖÊïàÊûúÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ã‰ª£Á†ÅÊõøÊç¢‰∏äÈù¢ÁöÑËΩÆÂªìÁªòÂà∂\n",
    "            # overlay = vis_image.copy()\n",
    "            # alpha = 0.5 # ÈÄèÊòéÂ∫¶\n",
    "            # overlay[combined_mask] = color_bgr\n",
    "            # vis_image = cv2.addWeighted(overlay, alpha, vis_image, 1 - alpha, 0)\n",
    "            \n",
    "        # if len(boxes) > 0:\n",
    "        #     for box in boxes:\n",
    "        #         x0, y0, x1, y1 = map(int, box)\n",
    "        #         # ÁªòÂà∂Ê£ÄÊµãÊ°Ü (ÁªøËâ≤)\n",
    "        #         cv2.rectangle(vis_image, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "    \n",
    "        # ‰ΩøÁî® Matplotlib ÊòæÁ§∫ÊúÄÁªàÁªìÊûú (Ê≥®ÊÑè OpenCV ÁöÑ BGR -> RGB ËΩ¨Êç¢)\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        # plt.imshow(cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.imshow(vis_image)\n",
    "        plt.title(f\"End-to-End Inference\\nDetected Objects: {len(boxes)}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. Ëá™Âä®ÂØªÊâæÊúÄÊñ∞ÁöÑÊùÉÈáçÊñá‰ª∂\n",
    "# =========================================================\n",
    "# Êâæ YOLO\n",
    "# Ê≥®ÊÑèÔºöË∑ØÂæÑÂèØËÉΩÈúÄË¶ÅÊ†πÊçÆ‰Ω†ÁöÑÂÆûÈôÖÊÉÖÂÜµÂæÆË∞É\n",
    "yolo_files = glob.glob(\"/root/task/src/YoloSAM/drive/runs/**/weights/best.pt\", recursive=True)\n",
    "if not yolo_files: raise FileNotFoundError(\"Êâæ‰∏çÂà∞‰ªª‰Ωï YOLO ÁöÑ best.ptÔºÅ\")\n",
    "latest_yolo_path = \"/root/task/src/YoloSAM/drive/runs/yolo_vessel_detection3/weights/best.pt\"\n",
    "print(f\"üëâ Found latest YOLO weights: {latest_yolo_path}\")\n",
    "\n",
    "# Êâæ SAM\n",
    "# Ê≥®ÊÑèÔºöË∑ØÂæÑÊòØ /root/autodl-tmp/run_*\n",
    "sam_files = glob.glob(\"/root/autodl-tmp/run_*/best_model.pth\", recursive=True)\n",
    "if not sam_files: raise FileNotFoundError(\"Êâæ‰∏çÂà∞‰ªª‰ΩïÂæÆË∞ÉÁöÑ SAM best_model.pthÔºÅ\")\n",
    "latest_sam_path = max(sam_files, key=os.path.getmtime)\n",
    "print(f\"üëâ Found latest SAM weights: {latest_sam_path}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. ÊâßË°åÊé®ÁêÜÂíåÂèØËßÜÂåñ\n",
    "# =========================================================\n",
    "# --- ÂàùÂßãÂåñÊé®ÁêÜÂô® ---\n",
    "pipeline = YoloSAMInference(\n",
    "    yolo_path=latest_yolo_path,\n",
    "    sam_path=latest_sam_path\n",
    ")\n",
    "\n",
    "# --- ÈÄâÊã©‰∏ÄÂº†ÂõæÁâáËøõË°åÈ¢ÑÊµã ---\n",
    "image_path = \"/root/task/datasets/DRIVE/val/images/01_test.tif\"\n",
    "print(f\"\\nüöÄ Predicting on image: {image_path}\")\n",
    "\n",
    "# --- ÊâßË°åÈ¢ÑÊµã ---\n",
    "results = pipeline.predict(image_path)\n",
    "\n",
    "# --- ÂèØËßÜÂåñÁªìÊûú ---\n",
    "print(f\"üìä YOLO detected {len(results['detected_boxes'])} objects.\")\n",
    "print(\"üé® Visualizing results...\")\n",
    "pipeline.visualize_results(results)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01multralytics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m YOLO\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# ÂØºÂÖ•‰Ω†È°πÁõÆ‰∏≠ÁöÑÊ®°Âùó\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mYoloSAM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msam\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SAMModel\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mYoloSAM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SAMFinetuneConfig, SAMDatasetConfig \u001B[38;5;66;03m# ÂÄüÁî®ConfigÊù•ÂàùÂßãÂåñÊ®°Âûã\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# =========================================================\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# 1. ÂÆö‰πâ‰∏Ä‰∏™‰∏ì‰∏öÁöÑ Inference Á±ª\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# =========================================================\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'src'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94876564-1c3c-42cb-96dd-aa718548821b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
