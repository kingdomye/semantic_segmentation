import sys
import os
import glob
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import cv2
from ultralytics import YOLO

# å¯¼å…¥ä½ é¡¹ç›®ä¸­çš„æ¨¡å—
from src.YoloSAM.models.sam import SAMModel
from src.YoloSAM.utils.config import SAMFinetuneConfig, SAMDatasetConfig  # å€Ÿç”¨Configæ¥åˆå§‹åŒ–æ¨¡å‹


# =========================================================
# 1. å®šä¹‰ä¸€ä¸ªä¸“ä¸šçš„ Inference ç±»
# =========================================================
class YoloSAMInference:
    def __init__(self, yolo_path, sam_path, device='mps'):
        self.device = torch.device(device)

        # --- 1. åŠ è½½ YOLO ---
        print(f"Loading YOLO from: {yolo_path}")
        self.yolo_model = YOLO(yolo_path)
        print("âœ… YOLO Model Loaded.")

        # --- 2. åŠ è½½ä½ å¾®è°ƒçš„ SAMModel ---
        print(f"Loading Fine-tuned SAM from: {sam_path}")
        self.sam_model = self._load_finetuned_sam(sam_path)
        self.sam_model.to(self.device)
        self.sam_model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        print("âœ… Fine-tuned SAM Model Loaded.")

    def _load_finetuned_sam(self, checkpoint_path):
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é…ç½®æ¥åˆå§‹åŒ–æ¨¡å‹éª¨æ¶
        # æ³¨æ„ï¼šè¿™é‡Œçš„ sam_path æ˜¯ä¸ºäº†åˆå§‹åŒ– SAMModel ç±»ï¼Œå®ƒä¸ä¼šè¢«å®é™…åŠ è½½
        config = SAMFinetuneConfig(model_type='vit_b', sam_path='./runs/sam_vit_b_01ec64.pth', device='mps')
        model = SAMModel(config)

        # åŠ è½½ä½ è®­ç»ƒå¥½çš„æƒé‡å­—å…¸
        state_dict = torch.load(checkpoint_path, map_location=self.device)

        # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœæƒé‡è¢«åŒ…è£¹åœ¨ 'model_state_dict' é‡Œï¼Œå…ˆæŠŠå®ƒå–å‡ºæ¥
        if 'model_state_dict' in state_dict:
            print("ğŸ“¦ Checkpoint format detected, extracting 'model_state_dict'...")
            state_dict = state_dict['model_state_dict']

        # åŠ è½½æƒé‡åˆ°æ¨¡å‹éª¨æ¶
        model.load_state_dict(state_dict)
        return model

    def predict(self, image_path, yolo_conf=0.01, yolo_iou=0.5, image_size=1024):
        # --- 1. å›¾åƒé¢„å¤„ç† ---
        image_pil = Image.open(image_path).convert("RGB")
        image_np = np.array(image_pil)

        # æ¨¡æ‹Ÿ Dataset ä¸­çš„ Resize
        original_shape = image_np.shape[:2]
        resized_image_np = cv2.resize(image_np, (image_size, image_size))

        # --- 2. YOLO æ¨ç† ---
        yolo_results = self.yolo_model.predict(resized_image_np, conf=yolo_conf, iou=yolo_iou, verbose=False)

        detected_boxes = []
        if yolo_results and len(yolo_results[0].boxes) > 0:
            detected_boxes = yolo_results[0].boxes.xyxy.cpu()  # è·å–æ‰€æœ‰æ£€æµ‹æ¡†

        # --- 3. SAM æ¨ç† (å¯¹æ¯ä¸ªæ£€æµ‹æ¡†) ---
        # å›¾åƒéœ€è¦è½¬æ¢ä¸º Tensor: (C, H, W) -> (1, C, H, W)
        image_tensor = torch.from_numpy(resized_image_np).permute(2, 0, 1).float() / 255.0
        image_tensor = image_tensor.to(self.device).unsqueeze(0)

        all_masks = []
        if len(detected_boxes) > 0:
            with torch.no_grad():
                for box in detected_boxes:
                    prompt_box = box.unsqueeze(0).to(self.device)  # (1, 4)

                    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„ forward æ–¹æ³•
                    pred_mask_logits, _ = self.sam_model.forward_one_image(
                        image=image_tensor,
                        bounding_box=prompt_box,
                        is_train=False
                    )

                    pred_mask_prob = torch.sigmoid(pred_mask_logits)
                    pred_mask_binary = (pred_mask_prob > 0.5).squeeze().cpu().numpy()
                    all_masks.append(pred_mask_binary)

        return {
            "original_image": resized_image_np,
            "detected_boxes": detected_boxes.numpy() if len(detected_boxes) > 0 else [],
            "predicted_masks": all_masks
        }

    # ++++++++++++++++++++ è¿™æ˜¯æ­£ç¡®çš„æ–°ç‰ˆæœ¬ï¼Œè¯·ä½¿ç”¨å®ƒ ++++++++++++++++++++
    def visualize_results(self, results):
        image = results['original_image']
        boxes = results['detected_boxes']
        masks = results['predicted_masks']

        # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ç”¨äºç»˜åˆ¶ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        vis_image = image.copy()

        if masks:
            # å°†æ‰€æœ‰ mask åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„å¸ƒå°”æ©ç 
            combined_mask = np.zeros_like(masks[0], dtype=bool)
            for mask in masks:
                # ç¡®ä¿ mask æ˜¯å¸ƒå°”ç±»å‹
                combined_mask = np.logical_or(combined_mask, mask.astype(bool))

            # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šåˆ›å»ºä¸€ä¸ªå½©è‰²çš„è¦†ç›–å±‚
            # å®šä¹‰é¢œè‰² (R, G, B)ï¼Œæ³¨æ„ OpenCV ä½¿ç”¨ BGR é¡ºåº
            color_bgr = (0, 255, 0)

            # å°†å¸ƒå°”æ©ç è½¬æ¢ä¸º uint8 æ ¼å¼ (0 æˆ– 255)
            binary_mask_uint8 = combined_mask.astype(np.uint8) * 255

            # ä½¿ç”¨ findContours æ‰¾åˆ°æ©ç çš„è½®å»“ï¼Œç»˜åˆ¶è½®å»“çº¿æ¯”å¡«å……æ›´æ¸…æ™°
            contours, _ = cv2.findContours(binary_mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(vis_image, contours, -1, color_bgr, thickness=2)  # ç»˜åˆ¶è½®å»“

            # å¦‚æœä½ æ›´å–œæ¬¢åŠé€æ˜å¡«å……æ•ˆæœï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ›¿æ¢ä¸Šé¢çš„è½®å»“ç»˜åˆ¶
            # overlay = vis_image.copy()
            # alpha = 0.5 # é€æ˜åº¦
            # overlay[combined_mask] = color_bgr
            # vis_image = cv2.addWeighted(overlay, alpha, vis_image, 1 - alpha, 0)

        # if len(boxes) > 0:
        #     for box in boxes:
        #         x0, y0, x1, y1 = map(int, box)
        #         # ç»˜åˆ¶æ£€æµ‹æ¡† (ç»¿è‰²)
        #         cv2.rectangle(vis_image, (x0, y0), (x1, y1), (0, 255, 0), 2)

        # ä½¿ç”¨ Matplotlib æ˜¾ç¤ºæœ€ç»ˆç»“æœ (æ³¨æ„ OpenCV çš„ BGR -> RGB è½¬æ¢)
        plt.figure(figsize=(12, 12))
        # plt.imshow(cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB))
        plt.imshow(vis_image)
        plt.title(f"End-to-End Inference\nDetected Objects: {len(boxes)}")
        plt.axis('off')
        plt.show()


# =========================================================
# 2. è‡ªåŠ¨å¯»æ‰¾æœ€æ–°çš„æƒé‡æ–‡ä»¶
# =========================================================
# æ‰¾ YOLO
# æ³¨æ„ï¼šè·¯å¾„å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„å®é™…æƒ…å†µå¾®è°ƒ
latest_yolo_path = "./runs/yolo_vessel_detection3/weights/best.pt"
print(f"ğŸ‘‰ Found latest YOLO weights: {latest_yolo_path}")

# æ‰¾ SAM
# æ³¨æ„ï¼šè·¯å¾„æ˜¯ /root/autodl-tmp/run_*
latest_sam_path = "./runs/run_25/best_model.pth"
print(f"ğŸ‘‰ Found latest SAM weights: {latest_sam_path}")

# =========================================================
# 3. æ‰§è¡Œæ¨ç†å’Œå¯è§†åŒ–
# =========================================================
# --- åˆå§‹åŒ–æ¨ç†å™¨ ---
pipeline = YoloSAMInference(
    yolo_path=latest_yolo_path,
    sam_path=latest_sam_path
)

# --- é€‰æ‹©ä¸€å¼ å›¾ç‰‡è¿›è¡Œé¢„æµ‹ ---
image_path = "../../../datasets/DRIVE/val/images/01_test.tif"
print(f"\nğŸš€ Predicting on image: {image_path}")

# --- æ‰§è¡Œé¢„æµ‹ ---
results = pipeline.predict(image_path)

# --- å¯è§†åŒ–ç»“æœ ---
print(f"ğŸ“Š YOLO detected {len(results['detected_boxes'])} objects.")
print("ğŸ¨ Visualizing results...")
pipeline.visualize_results(results)