{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002a4af-a6d4-4be7-9e49-991a5cbff125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‰ Found latest YOLO weights: /root/task/src/YoloSAM/drive/runs/yolo_vessel_detection3/weights/best.pt\n",
      "ğŸ‘‰ Found latest SAM weights: /root/autodl-tmp/run_25/best_model.pth\n",
      "Loading YOLO from: /root/task/src/YoloSAM/drive/runs/yolo_vessel_detection3/weights/best.pt\n",
      "âœ… YOLO Model Loaded.\n",
      "Loading Fine-tuned SAM from: /root/autodl-tmp/run_25/best_model.pth\n",
      "Load SAM model from  /root/task/checkpoints/sam_vit_b_01ec64.pth\n",
      "ğŸ“¦ Checkpoint format detected, extracting 'model_state_dict'...\n",
      "âœ… Fine-tuned SAM Model Loaded.\n",
      "\n",
      "ğŸ“Š å¼€å§‹è¯„ä¼°ï¼Œå…± 20 å¼ éªŒè¯å›¾ç‰‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:45<00:00,  2.24s/it]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "\n",
    "# =========================================================\n",
    "# 0. è·¯å¾„é…ç½®ä¸ç¯å¢ƒå‡†å¤‡\n",
    "# =========================================================\n",
    "paths_to_add = ['/root', '/root/task']\n",
    "for p in paths_to_add:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "# å¯¼å…¥é¡¹ç›®æ¨¡å—\n",
    "from src.YoloSAM.models.sam import SAMModel\n",
    "from src.YoloSAM.utils.config import SAMFinetuneConfig, SAMDatasetConfig\n",
    "\n",
    "# åˆ›å»ºè¯„ä¼°ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºå­ç›®å½•ï¼šmetrics/å¯è§†åŒ–/æŠ¥å‘Šï¼‰\n",
    "EVALUATE_DIR = \"/root/task/evaluate\"\n",
    "os.makedirs(EVALUATE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(EVALUATE_DIR, \"metrics\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(EVALUATE_DIR, \"visualizations\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(EVALUATE_DIR, \"reports\"), exist_ok=True)\n",
    "\n",
    "# æ•°æ®é›†è·¯å¾„ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰\n",
    "VAL_IMAGE_DIR = \"/root/task/datasets/DRIVE/val/images\"\n",
    "VAL_LABEL_DIR = \"/root/task/datasets/DRIVE/val/label\"  # maskç›®å½•åï¼šlabel\n",
    "IMAGE_EXT = \".tif\"\n",
    "LABEL_EXT = \".gif\"  # æ ‡ç­¾æ–‡ä»¶æ‰©å±•åï¼ˆæ ¹æ®å®é™…è°ƒæ•´ï¼‰\n",
    "LABEL_SUFFIX = \"manual1\"\n",
    "IMAGE_SIZE = 1024\n",
    "\n",
    "# =========================================================\n",
    "# 1. å®šä¹‰è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•°\n",
    "# =========================================================\n",
    "class SegmentationMetrics:\n",
    "    \"\"\"è¯­ä¹‰åˆ†å‰²è¯„ä¼°æŒ‡æ ‡è®¡ç®—ç±»\"\"\"\n",
    "    def __init__(self, eps=1e-6):\n",
    "        self.eps = eps  # é˜²æ­¢é™¤é›¶é”™è¯¯\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"é‡ç½®æ‰€æœ‰æŒ‡æ ‡\"\"\"\n",
    "        self.total_tp = 0  # çœŸé˜³æ€§\n",
    "        self.total_fp = 0  # å‡é˜³æ€§\n",
    "        self.total_fn = 0  # å‡é˜´æ€§\n",
    "        self.total_tn = 0  # çœŸé˜´æ€§\n",
    "        self.image_metrics = []  # å•å¼ å›¾ç‰‡çš„æŒ‡æ ‡\n",
    "\n",
    "    def calculate_single(self, pred_mask, gt_mask):\n",
    "        \"\"\"è®¡ç®—å•å¼ å›¾ç‰‡çš„æŒ‡æ ‡\"\"\"\n",
    "        # ç¡®ä¿æ©ç ä¸ºå¸ƒå°”ç±»å‹\n",
    "        pred_mask = pred_mask.astype(bool)\n",
    "        gt_mask = gt_mask.astype(bool)\n",
    "\n",
    "        # è®¡ç®—æ··æ·†çŸ©é˜µå…ƒç´ \n",
    "        tp = np.logical_and(pred_mask, gt_mask).sum()\n",
    "        fp = np.logical_and(pred_mask, np.logical_not(gt_mask)).sum()\n",
    "        fn = np.logical_and(np.logical_not(pred_mask), gt_mask).sum()\n",
    "        tn = np.logical_and(np.logical_not(pred_mask), np.logical_not(gt_mask)).sum()\n",
    "\n",
    "        # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡\n",
    "        iou = tp / (tp + fp + fn + self.eps)\n",
    "        dice = 2 * tp / (2 * tp + fp + fn + self.eps)\n",
    "        precision = tp / (tp + fp + self.eps)\n",
    "        recall = tp / (tp + fn + self.eps)\n",
    "        f1 = 2 * precision * recall / (precision + recall + self.eps)\n",
    "\n",
    "        return {\n",
    "            \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n",
    "            \"iou\": iou, \"dice\": dice,\n",
    "            \"precision\": precision, \"recall\": recall, \"f1\": f1\n",
    "        }\n",
    "\n",
    "    def update(self, pred_mask, gt_mask, image_name):\n",
    "        \"\"\"æ›´æ–°å…¨å±€æŒ‡æ ‡\"\"\"\n",
    "        metrics = self.calculate_single(pred_mask, gt_mask)\n",
    "        self.total_tp += metrics[\"tp\"]\n",
    "        self.total_fp += metrics[\"fp\"]\n",
    "        self.total_fn += metrics[\"fn\"]\n",
    "        self.total_tn += metrics[\"tn\"]\n",
    "        \n",
    "        metrics[\"image_name\"] = image_name\n",
    "        self.image_metrics.append(metrics)\n",
    "\n",
    "    def get_global_metrics(self):\n",
    "        \"\"\"è®¡ç®—å…¨å±€å¹³å‡æŒ‡æ ‡\"\"\"\n",
    "        # å…¨å±€æ··æ·†çŸ©é˜µï¼ˆè½¬æ¢ä¸ºPythonåŸç”Ÿintï¼‰\n",
    "        tp = int(self.total_tp)\n",
    "        fp = int(self.total_fp)\n",
    "        fn = int(self.total_fn)\n",
    "        tn = int(self.total_tn)\n",
    "    \n",
    "        # å…¨å±€æŒ‡æ ‡ï¼ˆä¿ç•™æµ®ç‚¹æ•°ï¼ŒJSONæ”¯æŒï¼‰\n",
    "        global_iou = tp / (tp + fp + fn + self.eps)\n",
    "        global_dice = 2 * tp / (2 * tp + fp + fn + self.eps)\n",
    "        global_precision = tp / (tp + fp + self.eps)\n",
    "        global_recall = tp / (tp + fn + self.eps)\n",
    "        global_f1 = 2 * global_precision * global_recall / (global_precision + global_recall + self.eps)\n",
    "    \n",
    "        return {\n",
    "            \"global_iou\": float(global_iou),  # æ˜¾å¼è½¬æ¢ä¸ºfloatï¼ˆå¯é€‰ï¼ŒJSONåŸç”Ÿæ”¯æŒï¼‰\n",
    "            \"global_dice\": float(global_dice),\n",
    "            \"global_precision\": float(global_precision),\n",
    "            \"global_recall\": float(global_recall),\n",
    "            \"global_f1\": float(global_f1),\n",
    "            \"confusion_matrix\": {\n",
    "                \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn  # å·²è½¬æ¢ä¸ºint\n",
    "            }\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# 2. æ”¹è¿›çš„æ¨ç†ç±»ï¼ˆæ”¯æŒè¯„ä¼°æ¨¡å¼ï¼‰\n",
    "# =========================================================\n",
    "class YoloSAMInference:\n",
    "    def __init__(self, yolo_path, sam_path, device='cuda'):\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        # åŠ è½½ YOLO\n",
    "        print(f\"Loading YOLO from: {yolo_path}\")\n",
    "        self.yolo_model = YOLO(yolo_path)\n",
    "        print(\"âœ… YOLO Model Loaded.\")\n",
    "        \n",
    "        # åŠ è½½å¾®è°ƒçš„ SAMModel\n",
    "        print(f\"Loading Fine-tuned SAM from: {sam_path}\")\n",
    "        self.sam_model = self._load_finetuned_sam(sam_path)\n",
    "        self.sam_model.to(self.device)\n",
    "        self.sam_model.eval()\n",
    "        print(\"âœ… Fine-tuned SAM Model Loaded.\")\n",
    "\n",
    "    def _load_finetuned_sam(self, checkpoint_path):\n",
    "        config = SAMFinetuneConfig(model_type='vit_b', sam_path='/root/task/checkpoints/sam_vit_b_01ec64.pth')\n",
    "        model = SAMModel(config)\n",
    "        \n",
    "        state_dict = torch.load(checkpoint_path, map_location=self.device)\n",
    "        if 'model_state_dict' in state_dict:\n",
    "            print(\"ğŸ“¦ Checkpoint format detected, extracting 'model_state_dict'...\")\n",
    "            state_dict = state_dict['model_state_dict']\n",
    "            \n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "\n",
    "    def predict(self, image_path, yolo_conf=0.01, yolo_iou=0.5, image_size=1024):\n",
    "        \"\"\"æ¨ç†å•å¼ å›¾ç‰‡ï¼Œè¿”å›é¢„æµ‹æ©ç ï¼ˆåˆå¹¶åï¼‰\"\"\"\n",
    "        # å›¾åƒé¢„å¤„ç†\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image_pil)\n",
    "        resized_image_np = cv2.resize(image_np, (image_size, image_size))\n",
    "        \n",
    "        # YOLO æ¨ç†\n",
    "        yolo_results = self.yolo_model.predict(resized_image_np, conf=yolo_conf, iou=yolo_iou, verbose=False)\n",
    "        detected_boxes = []\n",
    "        if yolo_results and len(yolo_results[0].boxes) > 0:\n",
    "            detected_boxes = yolo_results[0].boxes.xyxy.cpu()\n",
    "        \n",
    "        # SAM æ¨ç†\n",
    "        image_tensor = torch.from_numpy(resized_image_np).permute(2, 0, 1).float() / 255.0\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)\n",
    "\n",
    "        combined_mask = np.zeros((image_size, image_size), dtype=bool)\n",
    "        if len(detected_boxes) > 0:\n",
    "            with torch.no_grad():\n",
    "                for box in detected_boxes:\n",
    "                    prompt_box = box.unsqueeze(0).to(self.device)\n",
    "                    pred_mask_logits, _ = self.sam_model.forward_one_image(\n",
    "                        image=image_tensor,\n",
    "                        bounding_box=prompt_box,\n",
    "                        is_train=False\n",
    "                    )\n",
    "                    pred_mask_prob = torch.sigmoid(pred_mask_logits)\n",
    "                    pred_mask_binary = (pred_mask_prob > 0.5).squeeze().cpu().numpy()\n",
    "                    combined_mask = np.logical_or(combined_mask, pred_mask_binary)\n",
    "\n",
    "        return {\n",
    "            \"original_image\": resized_image_np,\n",
    "            \"detected_boxes\": detected_boxes.numpy() if len(detected_boxes) > 0 else [],\n",
    "            \"predicted_mask\": combined_mask,  # åˆå¹¶åçš„æ©ç \n",
    "            \"original_shape\": image_np.shape[:2]\n",
    "        }\n",
    "\n",
    "    def load_gt_mask(self, label_path, image_size=1024):\n",
    "        \"\"\"åŠ è½½å¹¶é¢„å¤„ç†çœŸå®æ ‡ç­¾æ©ç \"\"\"\n",
    "        gt_mask = Image.open(label_path).convert(\"L\")  # ç°åº¦å›¾\n",
    "        gt_mask = np.array(gt_mask)\n",
    "        # äºŒå€¼åŒ–ï¼ˆæ ¹æ®æ•°æ®é›†è°ƒæ•´é˜ˆå€¼ï¼ŒDRIVEé€šå¸¸æ˜¯0/255ï¼‰\n",
    "        gt_mask = (gt_mask > 127).astype(bool)\n",
    "        # è°ƒæ•´å°ºå¯¸ä¸é¢„æµ‹æ©ç ä¸€è‡´\n",
    "        gt_mask = cv2.resize(gt_mask.astype(np.uint8), (image_size, image_size)) > 0\n",
    "        return gt_mask\n",
    "\n",
    "    def visualize_evaluation(self, image_np, pred_mask, gt_mask, image_name):\n",
    "        \"\"\"å¯è§†åŒ–è¯„ä¼°ç»“æœï¼ˆåŸå›¾+é¢„æµ‹æ©ç +çœŸå®æ©ç +å¯¹æ¯”ï¼‰\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "        \n",
    "        # 1. åŸå›¾\n",
    "        axes[0,0].imshow(image_np)\n",
    "        axes[0,0].set_title(\"Original Image\")\n",
    "        axes[0,0].axis('off')\n",
    "        \n",
    "        # 2. é¢„æµ‹æ©ç \n",
    "        axes[0,1].imshow(pred_mask, cmap='gray')\n",
    "        axes[0,1].set_title(\"Predicted Mask\")\n",
    "        axes[0,1].axis('off')\n",
    "        \n",
    "        # 3. çœŸå®æ©ç \n",
    "        axes[1,0].imshow(gt_mask, cmap='gray')\n",
    "        axes[1,0].set_title(\"Ground Truth Mask\")\n",
    "        axes[1,0].axis('off')\n",
    "        \n",
    "        # 4. å¯¹æ¯”å›¾ï¼ˆTP:ç»¿, FP:çº¢, FN:é»„ï¼‰\n",
    "        compare_mask = np.zeros((*pred_mask.shape, 3), dtype=np.uint8)\n",
    "        compare_mask[np.logical_and(pred_mask, gt_mask)] = [0, 255, 0]    # TP: ç»¿è‰²\n",
    "        compare_mask[np.logical_and(pred_mask, ~gt_mask)] = [255, 0, 0]   # FP: çº¢è‰²\n",
    "        compare_mask[np.logical_and(~pred_mask, gt_mask)] = [255, 255, 0] # FN: é»„è‰²\n",
    "        axes[1,1].imshow(compare_mask)\n",
    "        axes[1,1].set_title(\"Comparison (TP:Green, FP:Red, FN:Yellow)\")\n",
    "        axes[1,1].axis('off')\n",
    "        \n",
    "        # ä¿å­˜å¯è§†åŒ–å›¾\n",
    "        save_path = os.path.join(EVALUATE_DIR, \"visualizations\", f\"{image_name}_eval.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# =========================================================\n",
    "# 3. ä¸»è¯„ä¼°æµç¨‹\n",
    "# =========================================================\n",
    "def main_evaluation():\n",
    "    # 1. åŠ è½½æƒé‡\n",
    "    # æ‰¾ YOLO\n",
    "    yolo_files = glob.glob(\"/root/task/src/YoloSAM/drive/runs/**/weights/best.pt\", recursive=True)\n",
    "    if not yolo_files: raise FileNotFoundError(\"æ‰¾ä¸åˆ°ä»»ä½• YOLO çš„ best.ptï¼\")\n",
    "    latest_yolo_path = \"/root/task/src/YoloSAM/drive/runs/yolo_vessel_detection3/weights/best.pt\"\n",
    "    print(f\"ğŸ‘‰ Found latest YOLO weights: {latest_yolo_path}\")\n",
    "\n",
    "    # æ‰¾ SAM\n",
    "    sam_files = glob.glob(\"/root/autodl-tmp/run_*/best_model.pth\", recursive=True)\n",
    "    if not sam_files: raise FileNotFoundError(\"æ‰¾ä¸åˆ°ä»»ä½•å¾®è°ƒçš„ SAM best_model.pthï¼\")\n",
    "    latest_sam_path = max(sam_files, key=os.path.getmtime)\n",
    "    print(f\"ğŸ‘‰ Found latest SAM weights: {latest_sam_path}\")\n",
    "\n",
    "    # 2. åˆå§‹åŒ–æ¨ç†å™¨å’ŒæŒ‡æ ‡è®¡ç®—å™¨\n",
    "    pipeline = YoloSAMInference(\n",
    "        yolo_path=latest_yolo_path,\n",
    "        sam_path=latest_sam_path,\n",
    "        device='cuda'\n",
    "    )\n",
    "    metrics_calculator = SegmentationMetrics()\n",
    "\n",
    "    # 3. è·å–éªŒè¯é›†å›¾ç‰‡åˆ—è¡¨\n",
    "    val_image_paths = glob.glob(os.path.join(VAL_IMAGE_DIR, f\"*{IMAGE_EXT}\"))\n",
    "    if not val_image_paths:\n",
    "        raise FileNotFoundError(f\"åœ¨ {VAL_IMAGE_DIR} ä¸­æœªæ‰¾åˆ° {IMAGE_EXT} æ ¼å¼çš„å›¾ç‰‡ï¼\")\n",
    "    print(f\"\\nğŸ“Š å¼€å§‹è¯„ä¼°ï¼Œå…± {len(val_image_paths)} å¼ éªŒè¯å›¾ç‰‡\")\n",
    "\n",
    "    # 4. æ‰¹é‡æ¨ç†+è¯„ä¼°\n",
    "    for image_path in tqdm(val_image_paths, desc=\"Evaluating\"):\n",
    "        # è·å–å›¾ç‰‡åç§°ï¼ˆç”¨äºä¿å­˜ï¼‰\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        \n",
    "        # å¯¹åº”çš„æ ‡ç­¾è·¯å¾„\n",
    "        label_path = os.path.join(VAL_LABEL_DIR, f\"{image_name}_{LABEL_SUFFIX}{LABEL_EXT}\")\n",
    "        label_path = label_path.replace('_test', '')\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"âš ï¸  è·³è¿‡ {image_name}ï¼šæœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶ {label_path}\")\n",
    "            continue\n",
    "\n",
    "        # æ¨ç†\n",
    "        results = pipeline.predict(image_path, image_size=IMAGE_SIZE)\n",
    "        pred_mask = results[\"predicted_mask\"]\n",
    "        \n",
    "        # åŠ è½½çœŸå®æ©ç \n",
    "        gt_mask = pipeline.load_gt_mask(label_path, image_size=IMAGE_SIZE)\n",
    "        \n",
    "        # è®¡ç®—å¹¶æ›´æ–°æŒ‡æ ‡\n",
    "        metrics_calculator.update(pred_mask, gt_mask, image_name)\n",
    "        \n",
    "        # å¯è§†åŒ–å¹¶ä¿å­˜\n",
    "        pipeline.visualize_evaluation(results[\"original_image\"], pred_mask, gt_mask, image_name)\n",
    "\n",
    "    # 5. è®¡ç®—å…¨å±€æŒ‡æ ‡\n",
    "    global_metrics = metrics_calculator.get_global_metrics()\n",
    "    print(\"\\n==================== å…¨å±€è¯„ä¼°æŒ‡æ ‡ ====================\")\n",
    "    print(f\"å…¨å±€ IoU: {global_metrics['global_iou']:.4f}\")\n",
    "    print(f\"å…¨å±€ Dice: {global_metrics['global_dice']:.4f}\")\n",
    "    print(f\"å…¨å±€ Precision: {global_metrics['global_precision']:.4f}\")\n",
    "    print(f\"å…¨å±€ Recall: {global_metrics['global_recall']:.4f}\")\n",
    "    print(f\"å…¨å±€ F1-Score: {global_metrics['global_f1']:.4f}\")\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    # 6. ä¿å­˜è¯„ä¼°ç»“æœ\n",
    "    # 6.1 ä¿å­˜å•å¼ å›¾ç‰‡æŒ‡æ ‡ï¼ˆCSVï¼‰\n",
    "    single_metrics_df = pd.DataFrame(metrics_calculator.image_metrics)\n",
    "    single_metrics_path = os.path.join(EVALUATE_DIR, \"metrics\", \"single_image_metrics.csv\")\n",
    "    single_metrics_df.to_csv(single_metrics_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 6.2 ä¿å­˜å…¨å±€æŒ‡æ ‡ï¼ˆJSONï¼‰\n",
    "    global_metrics_path = os.path.join(EVALUATE_DIR, \"reports\", \"global_metrics.json\")\n",
    "    with open(global_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(global_metrics, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # 6.3 ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–\n",
    "    cm = np.array([\n",
    "        [global_metrics['confusion_matrix']['tn'], global_metrics['confusion_matrix']['fp']],\n",
    "        [global_metrics['confusion_matrix']['fn'], global_metrics['confusion_matrix']['tp']]\n",
    "    ])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Ground Truth')\n",
    "    plt.title('Confusion Matrix (Global)')\n",
    "    cm_save_path = os.path.join(EVALUATE_DIR, \"reports\", \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 6.4 ç”ŸæˆæŒ‡æ ‡æ±‡æ€»æŠ¥å‘Šï¼ˆTXTï¼‰\n",
    "    report_path = os.path.join(EVALUATE_DIR, \"reports\", \"evaluation_report.txt\")\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"==================== YoloSAM æ¨¡å‹è¯„ä¼°æŠ¥å‘Š ====================\\n\")\n",
    "        f.write(f\"è¯„ä¼°æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"éªŒè¯é›†æ•°é‡: {len(val_image_paths)}\\n\")\n",
    "        f.write(f\"YOLO æƒé‡è·¯å¾„: {latest_yolo_path}\\n\")\n",
    "        f.write(f\"SAM æƒé‡è·¯å¾„: {latest_sam_path}\\n\")\n",
    "        f.write(\"\\nã€å…¨å±€æŒ‡æ ‡ã€‘\\n\")\n",
    "        f.write(f\"IoU: {global_metrics['global_iou']:.4f}\\n\")\n",
    "        f.write(f\"Dice ç³»æ•°: {global_metrics['global_dice']:.4f}\\n\")\n",
    "        f.write(f\"ç²¾ç¡®ç‡ (Precision): {global_metrics['global_precision']:.4f}\\n\")\n",
    "        f.write(f\"å¬å›ç‡ (Recall): {global_metrics['global_recall']:.4f}\\n\")\n",
    "        f.write(f\"F1 åˆ†æ•°: {global_metrics['global_f1']:.4f}\\n\")\n",
    "        f.write(\"\\nã€æ··æ·†çŸ©é˜µã€‘\\n\")\n",
    "        f.write(f\"çœŸé˜´æ€§ (TN): {global_metrics['confusion_matrix']['tn']}\\n\")\n",
    "        f.write(f\"å‡é˜³æ€§ (FP): {global_metrics['confusion_matrix']['fp']}\\n\")\n",
    "        f.write(f\"å‡é˜´æ€§ (FN): {global_metrics['confusion_matrix']['fn']}\\n\")\n",
    "        f.write(f\"çœŸé˜³æ€§ (TP): {global_metrics['confusion_matrix']['tp']}\\n\")\n",
    "        f.write(\"==============================================================\\n\")\n",
    "\n",
    "    print(f\"\\nâœ… è¯„ä¼°å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°ï¼š{EVALUATE_DIR}\")\n",
    "    print(f\"   - å•å¼ å›¾ç‰‡æŒ‡æ ‡: {single_metrics_path}\")\n",
    "    print(f\"   - å…¨å±€æŒ‡æ ‡: {global_metrics_path}\")\n",
    "    print(f\"   - è¯„ä¼°æŠ¥å‘Š: {report_path}\")\n",
    "    print(f\"   - æ··æ·†çŸ©é˜µ: {cm_save_path}\")\n",
    "    print(f\"   - å¯è§†åŒ–ç»“æœ: {os.path.join(EVALUATE_DIR, 'visualizations')}\")\n",
    "\n",
    "# =========================================================\n",
    "# æ‰§è¡Œè¯„ä¼°\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d2c13-93c4-412b-ab00-d92378a6621f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
